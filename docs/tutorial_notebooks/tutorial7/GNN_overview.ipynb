{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQdjIhptJSYt"
      },
      "source": [
        "# Tutorial 7: Graph Neural Networks\n",
        "\n",
        "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
        "\n",
        "**Filled notebook:**\n",
        "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/GNN_overview.ipynb)\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/GNN_overview.ipynb)  \n",
        "**Pre-trained models:**\n",
        "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/tutorial7)\n",
        "[![GoogleDrive](https://img.shields.io/static/v1.svg?logo=google-drive&logoColor=yellow&label=GDrive&message=Download&color=yellow)](https://drive.google.com/drive/folders/1DOTV_oYt5boa-MElbc2izat4VMSc1gob?usp=sharing)   \n",
        "**Recordings:**\n",
        "[![YouTube - Part 1](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%201&color=red)](https://youtu.be/fK7d56Ly9q8)\n",
        "[![YouTube - Part 2](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%202&color=red)](https://youtu.be/ZCNSUWe4a_Q)   \n",
        "**JAX+Flax version:**\n",
        "[![View on RTD](https://img.shields.io/static/v1.svg?logo=readthedocs&label=RTD&message=View%20On%20RTD&color=8CA1AF)](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial7/GNN_overview.html)   \n",
        "**Author:** Phillip Lippe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKi7XD7iJSYy"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "\n",
        "**Note:** Interested in JAX? Check out our [JAX+Flax version](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial7/GNN_overview.html) of this tutorial!\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jklMLf4JSY0"
      },
      "source": [
        "Trong bài hướng dẫn này, chúng ta sẽ thảo luận về ứng dụng của Neural Networks trên các Graph.\n",
        "\n",
        "Graph Neural Networks (GNNs) gần đây ngày càng trở nên phổ biến trong cả ứng dụng thực tế lẫn nghiên cứu, bao gồm các lĩnh vực như Social Networks (mạng xã hội), Knowledge Graphs (đồ thị tri thức), Recommender Systems (hệ thống gợi ý) và Bioinformatics (tin sinh học).\n",
        "\n",
        "Mặc dù lý thuyết và toán học đằng sau GNN thoạt nhìn có vẻ phức tạp, nhưng việc implementation (triển khai code) các model này lại khá đơn giản và hỗ trợ rất tốt cho việc hiểu rõ methodology (phương pháp luận).\n",
        "\n",
        "Do đó, chúng ta sẽ thảo luận về cách implementation các network layer cơ bản của một GNN, cụ thể là:\n",
        "\n",
        "Graph Convolutions\n",
        "\n",
        "Attention Layers\n",
        "\n",
        "Cuối cùng, chúng ta sẽ áp dụng GNN vào các task (tác vụ) ở các cấp độ:\n",
        "\n",
        "Node-level (cấp độ nút)\n",
        "\n",
        "Edge-level (cấp độ cạnh)\n",
        "\n",
        "Graph-level (cấp độ toàn bộ đồ thị)\n",
        "\n",
        "Bên dưới, chúng ta sẽ bắt đầu bằng việc import các thư viện tiêu chuẩn. Chúng ta sẽ sử dụng PyTorch Lightning như đã thực hiện trong Tutorial 5 và 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxchzGVmJSY0",
        "outputId": "2013df86-ed47-4f5e-ee99-2ebdd4f1bc3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-615682122.py:12: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg', 'pdf') # Để export (xuất file ảnh chất lượng cao)\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "## Các thư viện Standard (tiêu chuẩn)\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "## Imports cho việc plotting (vẽ biểu đồ)\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # Để export (xuất file ảnh chất lượng cao)\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "sns.set()\n",
        "\n",
        "## Progress bar (Thanh tiến độ)\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab không cài đặt sẵn PyTorch Lightning theo mặc định. Do đó, chúng ta cài đặt nó ở đây nếu cần thiết\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Đường dẫn đến thư mục nơi các dataset đang có hoặc sẽ được download (ví dụ: CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Đường dẫn đến thư mục nơi các pretrained model được lưu\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial7\"\n",
        "\n",
        "# Thiết lập seed (để cố định kết quả ngẫu nhiên)\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Đảm bảo rằng tất cả các operation là deterministic trên GPU (nếu được sử dụng) cho mục đích reproducibility (khả năng tái lập kết quả)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx7_RME4JSY3"
      },
      "source": [
        "chúng ta cũng có một số pre-trained model ở dưới"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybVj5JCNJSY4",
        "outputId": "1532ee7c-63f7-4f8d-9ab4-ab75d61883a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đang download https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/NodeLevelMLP.ckpt...\n",
            "Đang download https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/NodeLevelGNN.ckpt...\n",
            "Đang download https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/GraphLevelGraphConv.ckpt...\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL nơi các saved models được lưu trữ cho tutorial này\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/\"\n",
        "# Các file cần download\n",
        "pretrained_files = [\"NodeLevelMLP.ckpt\", \"NodeLevelGNN.ckpt\", \"GraphLevelGraphConv.ckpt\"]\n",
        "\n",
        "# Tạo checkpoint path nếu nó chưa tồn tại\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# Với mỗi file, kiểm tra xem nó đã tồn tại chưa. Nếu chưa, thử download nó.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if \"/\" in file_name:\n",
        "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Đang download {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Có lỗi xảy ra. Vui lòng thử download file từ GDrive folder, hoặc liên hệ tác giả với full output bao gồm error sau đây:\\n\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axYW95O_JSY5"
      },
      "source": [
        "## Graph Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Wx7iz0JSY5"
      },
      "source": [
        "### Graph representation (Biểu diễn Graph)\n",
        "\n",
        "Trước khi bắt đầu thảo luận về các hoạt động cụ thể của neural network trên các graph, chúng ta nên xem xét cách để biểu diễn một graph. Về mặt toán học, một graph $\\mathcal{G}$ được định nghĩa là một tuple (bộ) bao gồm một set (tập hợp) các nodes/vertices $V$, và một set các edges/links $E$: $\\mathcal{G}=(V,E)$. Mỗi edge là một cặp gồm hai vertices, và đại diện cho một kết nối giữa chúng. Ví dụ, hãy nhìn vào graph sau đây:\n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/example_graph.svg?raw=1\" width=\"250px\"></center>\n",
        "\n",
        "Các vertices là $V=\\{1,2,3,4\\}$, và các edges là $E=\\{(1,2), (2,3), (2,4), (3,4)\\}$. Lưu ý rằng để đơn giản hóa, chúng ta giả định graph là undirected (vô hướng) và do đó không thêm các cặp đối xứng (mirrored pairs) như $(2,1)$. Trong ứng dụng thực tế, các vertices và edge thường có thể có các attributes cụ thể, và các edges thậm chí có thể là directed (có hướng).\n",
        "\n",
        "Câu hỏi đặt ra là làm thế nào chúng ta có thể biểu diễn sự đa dạng này một cách hiệu quả cho các matrix operations. Thông thường, đối với các edges, chúng ta quyết định giữa hai biến thể: một **adjacency matrix**, hoặc một **list of paired vertex indices**.\n",
        "\n",
        "**Adjacency matrix** $A$ là một square matrix mà các phần tử của nó chỉ ra liệu các cặp vertices có adjacent (kề nhau), tức là có được connected hay không. Trong trường hợp đơn giản nhất, $A_{ij}$ là 1 nếu có một connection từ node $i$ đến $j$, và ngược lại là 0. Nếu chúng ta có edge attributes hoặc các danh mục edges khác nhau trong một graph, thông tin này cũng có thể được thêm vào matrix. Đối với một undirected graph, hãy nhớ rằng $A$ là một symmetric matrix ($A_{ij}=A_{ji}$). Đối với graph ví dụ ở trên, chúng ta có adjacency matrix sau:\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix}\n",
        "    0 & 1 & 0 & 0\\\\\n",
        "    1 & 0 & 1 & 1\\\\\n",
        "    0 & 1 & 0 & 1\\\\\n",
        "    0 & 1 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Mặc dù việc biểu diễn một graph dưới dạng một list of edges sẽ hiệu quả hơn về mặt memory và (có thể) computation, nhưng việc sử dụng một adjacency matrix lại trực quan hơn và đơn giản hơn để implement. Trong các implementation bên dưới, chúng ta sẽ dựa vào adjacency matrix để giữ cho code đơn giản. Tuy nhiên, các thư viện phổ biến (common libraries) thường sử dụng edge lists, điều mà chúng ta sẽ thảo luận thêm sau này.\n",
        "\n",
        "Ngoài ra, chúng ta cũng có thể sử dụng list of edges để định nghĩa một **sparse adjacency matrix**, cho phép chúng ta làm việc với nó như thể nó là một **dense matrix**, nhưng cho phép các operations hiệu quả hơn về bộ nhớ. PyTorch hỗ trợ việc này với sub-package `torch.sparse` ([tài liệu](https://pytorch.org/docs/stable/sparse.html)), tuy nhiên nó vẫn đang trong giai đoạn beta-stage (API có thể thay đổi trong tương lai)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIf26oRdJSY6"
      },
      "source": [
        "### Graph Convolutions\n",
        "\n",
        "**Graph Convolutional Networks (GCNs)** đã được giới thiệu bởi [Kipf et al.](https://openreview.net/pdf?id=SJU4ayYgl) vào năm 2016 tại Đại học Amsterdam. Ông cũng đã viết một [blog post](https://tkipf.github.io/graph-convolutional-networks/) rất tuyệt về chủ đề này, bài viết được recommended nếu bạn muốn tìm hiểu về GCNs từ một góc nhìn khác. GCNs tương tự như **convolutions** trong xử lý ảnh ở chỗ các tham số \"**filter**\" thường được **shared** (chia sẻ) trên tất cả các vị trí trong graph. Đồng thời, GCNs dựa vào các phương pháp **message passing**, nghĩa là các vertices trao đổi thông tin với các **neighbors** (hàng xóm), và gửi \"**messages**\" (thông điệp) cho nhau.\n",
        "\n",
        "Trước khi xem xét về toán học, chúng ta có thể thử tìm hiểu cách GCNs hoạt động một cách trực quan.\n",
        "* **Bước 1:** Mỗi node tạo ra một **feature vector** đại diện cho **message** mà nó muốn gửi đến tất cả các neighbors của nó.\n",
        "* **Bước 2:** Các messages được gửi đến các neighbors, sao cho một node nhận được một message từ mỗi **adjacent node**.\n",
        "\n",
        "Bên dưới, chúng ta đã trực quan hóa hai bước này cho graph ví dụ của mình.\n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/graph_message_passing.svg?raw=1\" width=\"700px\"></center>\n",
        "\n",
        "Nếu chúng ta muốn công thức hóa điều đó bằng các thuật ngữ toán học, trước tiên chúng ta cần quyết định cách kết hợp tất cả các messages mà một node nhận được. Vì số lượng messages thay đổi tùy theo các nodes (do số lượng hàng xóm khác nhau), chúng ta cần một **operation** hoạt động được với bất kỳ số lượng nào. Do đó, cách thông thường là tính tổng (sum) hoặc lấy trung bình (mean).\n",
        "\n",
        "Với các **features** trước đó của các nodes là $H^{(l)}$, lớp **GCN layer** được định nghĩa như sau:\n",
        "\n",
        "$$H^{(l+1)} = \\sigma\\left(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{(l)}W^{(l)}\\right)$$\n",
        "\n",
        "Trong đó:\n",
        "* $W^{(l)}$ là các tham số **weight** mà chúng ta dùng để biến đổi các **input features** thành các messages ($H^{(l)}W^{(l)}$).\n",
        "* Đối với **adjacency matrix** $A$, chúng ta cộng thêm **identity matrix** (ma trận đơn vị) để mỗi node cũng tự gửi message của chính nó cho nó: $\\hat{A}=A+I$.\n",
        "* Cuối cùng, để lấy trung bình thay vì tính tổng, chúng ta tính toán **matrix** $\\hat{D}$, đây là một **diagonal matrix** (ma trận đường chéo) với $D_{ii}$ biểu thị số lượng neighbors mà node $i$ có.\n",
        "* $\\sigma$ đại diện cho một **activation function** bất kỳ, và không nhất thiết phải là sigmoid (thường thì **ReLU-based activation function** được sử dụng trong GNNs).\n",
        "\n",
        "Khi **implement** lớp GCN layer trong PyTorch, chúng ta có thể tận dụng các **operations** linh hoạt trên các **tensors**. Thay vì định nghĩa một matrix $\\hat{D}$ (việc này tốn kém tính toán), chúng ta có thể chỉ cần chia các messages đã được tính tổng cho số lượng neighbors sau đó. Ngoài ra, chúng ta thay thế weight matrix bằng một **linear layer**, điều này cho phép chúng ta thêm vào một **bias**.\n",
        "\n",
        "Được viết dưới dạng một **PyTorch module**, lớp GCN layer được định nghĩa như sau:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzUKu0wZJSY7"
      },
      "outputs": [],
      "source": [
        "class GCNLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super().__init__()\n",
        "        # Lớp Linear này đóng vai trò là ma trận trọng số W trong công thức\n",
        "        self.projection = nn.Linear(c_in, c_out)\n",
        "\n",
        "    def forward(self, node_feats, adj_matrix):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            node_feats - Tensor chứa các node features có shape [batch_size, num_nodes, c_in]\n",
        "            adj_matrix - Batch các adjacency matrices của graph. Nếu có edge từ i đến j, adj_matrix[b,i,j]=1 ngược lại là 0.\n",
        "                         Hỗ trợ các directed edges bằng các non-symmetric matrices.\n",
        "                         Giả định rằng đã thêm các identity connections (tự kết nối với chính nó).\n",
        "                         Shape: [batch_size, num_nodes, num_nodes]\n",
        "        \"\"\"\n",
        "        # Num neighbours = số lượng incoming edges (cạnh đi vào)\n",
        "        # Sum theo dimension cuối cùng để đếm xem mỗi node có bao nhiêu hàng xóm kết nối tới nó\n",
        "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
        "\n",
        "        # Bước 1: Biến đổi features (tương ứng với H * W trong công thức)\n",
        "        node_feats = self.projection(node_feats)\n",
        "\n",
        "        # Bước 2: Message Passing & Aggregation\n",
        "        # Nhân ma trận kề với node features (bmm = batch matrix multiplication)\n",
        "        # Thao tác này thực chất là cộng tổng features của các neighbors\n",
        "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
        "\n",
        "        # Bước 3: Normalize (Chuẩn hóa)\n",
        "        # Chia cho số lượng neighbors để lấy trung bình (thay vì chỉ lấy tổng)\n",
        "        node_feats = node_feats / num_neighbours\n",
        "\n",
        "        return node_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frbY0w8uJSY7"
      },
      "source": [
        "Để hiểu sâu hơn về **GCN layer**, chúng ta có thể áp dụng nó vào **example graph** ở trên. Đầu tiên, hãy xác định một số **node features** và **adjacency matrix** với các **self-connections** đã được thêm vào:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGBgegFQJSY8",
        "outputId": "9b2ea4a9-17af-4ebd-bbff-526109ccc320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node features:\n",
            " tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "\n",
            "Adjacency matrix:\n",
            " tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n"
          ]
        }
      ],
      "source": [
        "# Tạo dummy node features (dữ liệu giả lập)\n",
        "# torch.arange(8) tạo ra các số từ 0 đến 7\n",
        "# .view(1, 4, 2) định hình lại tensor thành: [Batch size=1, Num nodes=4, Num features=2]\n",
        "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
        "\n",
        "# Định nghĩa Adjacency Matrix (Ma trận kề)\n",
        "# Lưu ý: Các phần tử trên đường chéo (diagonal) đều là 1\n",
        "# Điều này có nghĩa là chúng ta đã thêm self-connections (mỗi node tự kết nối với chính nó)\n",
        "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
        "                            [1, 1, 1, 1],\n",
        "                            [0, 1, 1, 1],\n",
        "                            [0, 1, 1, 1]]])\n",
        "\n",
        "print(\"Node features:\\n\", node_feats)\n",
        "print(\"\\nAdjacency matrix:\\n\", adj_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tk-pMFqJSY8"
      },
      "source": [
        "Tiếp theo, hãy áp dụng một **GCN layer** vào nó. Để đơn giản, chúng ta khởi tạo **linear weight matrix** như là một **identity matrix** để các **input features** bằng với các **messages**. Điều này giúp chúng ta dễ dàng xác minh **message passing operation** hơn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFxxKrkPJSY8",
        "outputId": "b2e2242d-f865-49f7-81cb-dfb4b7d8983e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n",
            "Input features tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "Output features tensor([[[1., 2.],\n",
            "         [3., 4.],\n",
            "         [4., 5.],\n",
            "         [4., 5.]]])\n"
          ]
        }
      ],
      "source": [
        "# Khởi tạo GCNLayer với input channels = 2 và output channels = 2\n",
        "layer = GCNLayer(c_in=2, c_out=2)\n",
        "\n",
        "# Gán cứng trọng số (weight) thành Ma trận đơn vị (Identity Matrix)\n",
        "# [[1., 0.], [0., 1.]] có nghĩa là features sẽ không bị thay đổi khi đi qua Linear layer\n",
        "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
        "\n",
        "# Gán cứng bias thành 0\n",
        "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
        "\n",
        "# Thực hiện forward pass mà không tính gradient (vì ta chỉ đang kiểm tra kết quả, không phải training)\n",
        "with torch.no_grad():\n",
        "    out_feats = layer(node_feats, adj_matrix)\n",
        "\n",
        "print(\"Adjacency matrix\", adj_matrix)\n",
        "print(\"Input features\", node_feats)\n",
        "print(\"Output features\", out_feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB1C6ndqJSY9"
      },
      "source": [
        "Như chúng ta có thể thấy, các **output values** của **node** đầu tiên là trung bình cộng của chính nó và **node** thứ hai. Tương tự, chúng ta có thể xác minh cho tất cả các **node** khác.\n",
        "\n",
        "Tuy nhiên, trong một **GNN**, chúng ta cũng muốn cho phép việc **feature exchange** (trao đổi đặc trưng) giữa các **nodes** nằm ngoài phạm vi **neighbors** (hàng xóm) trực tiếp của nó. Điều này có thể đạt được bằng cách áp dụng nhiều **GCN layers**, tạo nên **final layout** (bố cục cuối cùng) của một **GNN**. Một **GNN** có thể được xây dựng bởi một chuỗi các **GCN layers** và các **non-linearities** (hàm phi tuyến) chẳng hạn như **ReLU**. Để hình dung, hãy xem bên dưới (nguồn ảnh - [Thomas Kipf, 2016](https://tkipf.github.io/graph-convolutional-networks/)).\n",
        "\n",
        "<center width=\"100%\" style=\"padding: 10px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/gcn_network.png?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "Tuy nhiên, một vấn đề mà chúng ta có thể thấy từ việc nhìn vào ví dụ trên là các **output features** cho **nodes** 3 và 4 giống hệt nhau bởi vì chúng có cùng các **adjacent nodes** (bao gồm cả chính nó). Do đó, các **GCN layers** có thể làm cho mạng quên đi các **node-specific information** (thông tin đặc thù của node) nếu chúng ta chỉ lấy trung bình trên tất cả các **messages**.\n",
        "\n",
        "Nhiều cải tiến khả thi đã được đề xuất. Trong khi lựa chọn đơn giản nhất có thể là sử dụng **residual connections**, cách tiếp cận phổ biến hơn là hoặc đặt trọng số cho các **self-connections** cao hơn, hoặc định nghĩa một **weight matrix** riêng biệt cho các **self-connections**.\n",
        "\n",
        "Ngoài ra, chúng ta có thể xem lại một khái niệm từ bài hướng dẫn trước: **attention**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojVSkSRMJSY9"
      },
      "source": [
        "### Graph Attention\n",
        "\n",
        "Nếu bạn còn nhớ bài hướng dẫn trước, **attention** mô tả một **weighted average** (trung bình có trọng số) của nhiều phần tử, với các trọng số được tính toán động (dynamically computed) dựa trên một **input query** và các **keys** của phần tử (nếu bạn chưa đọc Tutorial 6, bạn nên xem qua ít nhất phần đầu tiên có tên [What is Attention?](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html#What-is-Attention?)).\n",
        "\n",
        "Khái niệm này có thể được áp dụng tương tự cho các **graphs**, một trong số đó là **Graph Attention Network** (được gọi là **GAT**, đề xuất bởi [Velickovic et al., 2017](https://arxiv.org/abs/1710.10903)).\n",
        "\n",
        "Tương tự như **GCN**, lớp **graph attention layer** tạo ra một **message** cho mỗi **node** sử dụng một **linear layer/weight matrix**. Đối với phần **attention**, nó sử dụng **message** từ chính **node** đó đóng vai trò là **query**, và các **messages** cần tính trung bình đóng vai trò là cả **keys** và **values** (lưu ý rằng điều này bao gồm cả **message** gửi cho chính nó).\n",
        "\n",
        "Hàm tính điểm (Score function) $f_{attn}$ được **implemented** như là một **one-layer MLP** giúp ánh xạ **query** và **key** thành một giá trị đơn lẻ. **MLP** trông như sau (nguồn ảnh - [Velickovic et al.](https://arxiv.org/abs/1710.10903)):\n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/graph_attention_MLP.svg?raw=1\" width=\"250px\"></center>\n",
        "\n",
        "Trong đó:\n",
        "* $h_i$ và $h_j$ là các **features** gốc từ **node** $i$ và $j$, và đại diện cho các **messages** của **layer** với $\\mathbf{W}$ là **weight matrix**.\n",
        "* $\\mathbf{a}$ là **weight matrix** của **MLP**, có **shape** $[1,2\\times d_{\\text{message}}]$.\n",
        "* $\\alpha_{ij}$ là **attention weight** cuối cùng từ **node** $i$ đến $j$.\n",
        "\n",
        "Việc tính toán có thể được mô tả như sau:\n",
        "\n",
        "$$\\alpha_{ij} = \\frac{\\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_j\\right]\\right)\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_k\\right]\\right)\\right)}$$\n",
        "\n",
        "Toán tử $||$ đại diện cho sự **concatenation** (phép nối), và $\\mathcal{N}_i$ là các chỉ số (indices) của các **neighbors** của **node** $i$.\n",
        "\n",
        "Lưu ý rằng trái ngược với thực hành thông thường, chúng ta áp dụng một **non-linearity** (ở đây là **LeakyReLU**) *trước* khi **softmax** trên các phần tử. Mặc dù thoạt đầu nó có vẻ như một thay đổi nhỏ, nhưng nó rất quan trọng để **attention** phụ thuộc vào **input** gốc. Cụ thể, hãy thử loại bỏ **non-linearity** trong một giây, và thử đơn giản hóa biểu thức:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\alpha_{ij} & = \\frac{\\exp\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_j\\right]\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_k\\right]\\right)}\\\\[5pt]\n",
        "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i+\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i+\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\[5pt]\n",
        "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i\\right)\\cdot\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i\\right)\\cdot\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\[5pt]\n",
        "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Chúng ta có thể thấy rằng nếu không có **non-linearity**, thành phần **attention** với $h_i$ thực sự tự triệt tiêu lẫn nhau, dẫn đến việc **attention** trở nên độc lập với chính **node** đó (node $i$). Do đó, chúng ta sẽ gặp phải vấn đề tương tự như **GCN** là tạo ra các **output features** giống nhau cho các **nodes** có cùng **neighbors**. Đây là lý do tại sao **LeakyReLU** là cốt yếu và giúp thêm sự phụ thuộc vào $h_i$ cho **attention**.\n",
        "\n",
        "Một khi chúng ta có được tất cả các **attention factors**, chúng ta có thể tính toán **output features** cho mỗi **node** bằng cách thực hiện **weighted average**:\n",
        "\n",
        "$$h_i'=\\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}\\mathbf{W}h_j\\right)$$\n",
        "\n",
        "$\\sigma$ lại là một **non-linearity** khác, giống như trong **GCN layer**. Về mặt hình ảnh, chúng ta có thể biểu diễn toàn bộ quá trình **message passing** trong một **attention layer** như sau (nguồn ảnh - [Velickovic et al.](https://arxiv.org/abs/1710.10903)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/graph_attention.jpeg?raw=1\" width=\"400px\"></center>\n",
        "\n",
        "Để tăng tính **expressiveness** (khả năng biểu diễn) của **graph attention network**, [Velickovic et al.](https://arxiv.org/abs/1710.10903) đã đề xuất mở rộng nó thành nhiều **heads** (đầu) tương tự như khối **Multi-Head Attention** trong **Transformers**. Điều này dẫn đến việc $N$ **attention layers** được áp dụng song song (**in parallel**). Trong hình ảnh trên, nó được trực quan hóa bằng ba màu mũi tên khác nhau (xanh lá, xanh dương và tím) mà sau đó được nối lại (**concatenated**). Việc lấy trung bình chỉ được áp dụng cho lớp **prediction layer** cuối cùng trong mạng.\n",
        "\n",
        "Sau khi đã thảo luận chi tiết về **graph attention layer**, chúng ta có thể **implement** nó bên dưới:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IxjIKY_JSY-"
      },
      "outputs": [],
      "source": [
        "class GATLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimensionality (số chiều) của input features\n",
        "            c_out - Dimensionality (số chiều) của output features\n",
        "            num_heads - Số lượng heads, tức là các cơ chế attention được áp dụng song song (in parallel).\n",
        "                        Các output features được chia đều cho các heads nếu concat_heads=True.\n",
        "            concat_heads - Nếu True, output của các heads khác nhau sẽ được nối lại (concatenated) thay vì lấy trung bình (averaged).\n",
        "            alpha - Negative slope của LeakyReLU activation.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.concat_heads = concat_heads\n",
        "        if self.concat_heads:\n",
        "            assert c_out % num_heads == 0, \"Số lượng output features phải là bội số của số lượng heads.\"\n",
        "            c_out = c_out // num_heads\n",
        "\n",
        "        # Các sub-modules và tham số cần thiết trong layer\n",
        "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
        "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out)) # Mỗi head có một vector trọng số a riêng\n",
        "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
        "\n",
        "        # Initialization (Khởi tạo) từ bản implementation gốc\n",
        "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            node_feats - Input features của node. Shape: [batch_size, c_in]\n",
        "            adj_matrix - Adjacency matrix bao gồm cả self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
        "            print_attn_probs - Nếu True, các attention weights sẽ được in ra trong quá trình forward pass (để debug)\n",
        "        \"\"\"\n",
        "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
        "\n",
        "        # Áp dụng linear layer và sắp xếp nodes theo head\n",
        "        node_feats = self.projection(node_feats)\n",
        "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
        "\n",
        "        # Chúng ta cần tính toán attention logits cho mọi edge trong adjacency matrix\n",
        "        # Làm việc này trên tất cả các tổ hợp nodes có thể (N x N) rất tốn kém\n",
        "        # => Tạo một tensor [W*h_i || W*h_j] với i và j là các chỉ số (indices) của tất cả các edges\n",
        "        edges = adj_matrix.nonzero(as_tuple=False) # Trả về các chỉ số nơi adjacency matrix khác 0 => edges\n",
        "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
        "\n",
        "        # Tính toán chỉ số phẳng (flat index) để lấy feature của node nguồn và node đích\n",
        "        edge_indices_row = edges[:,0] * num_nodes + edges[:,1]\n",
        "        edge_indices_col = edges[:,0] * num_nodes + edges[:,2]\n",
        "\n",
        "        # Nối (Concatenate) features của cặp node (i, j) lại với nhau\n",
        "        a_input = torch.cat([\n",
        "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
        "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
        "        ], dim=-1) # Index select trả về một tensor với node_feats_flat được đánh chỉ mục tại các vị trí mong muốn dọc theo dim=0\n",
        "\n",
        "        # Tính toán output của attention MLP (độc lập cho mỗi head)\n",
        "        # einsum: thực hiện phép nhân ma trận hiệu quả\n",
        "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
        "        attn_logits = self.leakyrelu(attn_logits)\n",
        "\n",
        "        # Map danh sách các giá trị attention ngược lại vào một matrix\n",
        "        # Khởi tạo matrix với giá trị cực nhỏ (-9e15) để khi qua Softmax nó sẽ xấp xỉ 0 (masking)\n",
        "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
        "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
        "\n",
        "        # Weighted average của attention (Tính Softmax)\n",
        "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
        "        if print_attn_probs:\n",
        "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
        "\n",
        "        # Tổng hợp features theo trọng số attention\n",
        "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
        "\n",
        "        # Nếu concat heads, chúng ta thực hiện reshape. Ngược lại, lấy trung bình (mean)\n",
        "        if self.concat_heads:\n",
        "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
        "        else:\n",
        "            node_feats = node_feats.mean(dim=2)\n",
        "\n",
        "        return node_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVpC2ajWJSY_"
      },
      "source": [
        "Một lần nữa, chúng ta có thể áp dụng **graph attention layer** vào **example graph** ở trên của chúng ta để hiểu rõ hơn về các cơ chế hoạt động (**dynamics**).\n",
        "\n",
        "Như trước đây, **input layer** được khởi tạo là một **identity matrix** (ma trận đơn vị), nhưng chúng ta thiết lập $\\mathbf{a}$ là một vector chứa các số bất kỳ để thu được các giá trị **attention** khác nhau. Chúng ta sử dụng hai **heads** để minh họa các cơ chế **attention** song song (**parallel**) và độc lập đang hoạt động trong **layer**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoLSfNegJSY_",
        "outputId": "7a4cf24d-8872-4e1e-ff4c-54e563eb3089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention probs\n",
            " tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
            "          [0.1096, 0.1450, 0.2642, 0.4813],\n",
            "          [0.0000, 0.1858, 0.2885, 0.5257],\n",
            "          [0.0000, 0.2391, 0.2696, 0.4913]],\n",
            "\n",
            "         [[0.5100, 0.4900, 0.0000, 0.0000],\n",
            "          [0.2975, 0.2436, 0.2340, 0.2249],\n",
            "          [0.0000, 0.3838, 0.3142, 0.3019],\n",
            "          [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
            "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n",
            "Input features tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "Output features tensor([[[1.2913, 1.9800],\n",
            "         [4.2344, 3.7725],\n",
            "         [4.6798, 4.8362],\n",
            "         [4.5043, 4.7351]]])\n"
          ]
        }
      ],
      "source": [
        "# Khởi tạo GATLayer\n",
        "# c_in=2, c_out=2, num_heads=2\n",
        "# Vì concat_heads=True (mặc định), mỗi head sẽ phụ trách output ra 1 feature (2 feature tổng / 2 heads)\n",
        "layer = GATLayer(2, 2, num_heads=2)\n",
        "\n",
        "# Gán trọng số projection là Ma trận đơn vị (Identity Matrix)\n",
        "# Điều này giúp features không bị biến đổi giá trị khi đi qua Linear layer (để dễ kiểm tra tính toán)\n",
        "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
        "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
        "\n",
        "# Gán trọng số a (attention mechanism parameters) thủ công\n",
        "# Chúng ta có 2 heads, nên tensor có shape tương ứng\n",
        "# Head 1: [-0.2, 0.3] -> Sẽ có cách \"chú ý\" riêng\n",
        "# Head 2: [0.1, -0.1] -> Sẽ có cách \"chú ý\" khác hẳn Head 1\n",
        "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.1, -0.1]])\n",
        "\n",
        "# Thực hiện forward pass không tính gradient\n",
        "with torch.no_grad():\n",
        "    # print_attn_probs=True sẽ kích hoạt dòng lệnh in ra ma trận attention trong hàm forward\n",
        "    out_feats = layer(node_feats, adj_matrix, print_attn_probs=True)\n",
        "\n",
        "print(\"Adjacency matrix\", adj_matrix)\n",
        "print(\"Input features\", node_feats)\n",
        "print(\"Output features\", out_feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4fIaJa9JSY_"
      },
      "source": [
        "Chúng tôi khuyến khích bạn thử tự tính toán **attention matrix** cho ít nhất một **head** và một **node**. Các giá trị sẽ là 0 tại những nơi không tồn tại **edge** giữa $i$ và $j$.\n",
        "\n",
        "Đối với các vị trí khác, chúng ta thấy một tập hợp đa dạng các **attention probabilities**. Hơn nữa, các **output features** của **node** 3 và 4 bây giờ đã khác nhau mặc dù chúng có cùng các **neighbors**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBD2JFkSJSZA"
      },
      "source": [
        "## PyTorch Geometric\n",
        "\n",
        "Chúng ta đã đề cập trước đó rằng việc **implementing graph networks** với **adjacency matrix** thì đơn giản và trực quan nhưng có thể **computationally expensive** (tốn kém về mặt tính toán) cho các **large graphs**.\n",
        "\n",
        "Nhiều **real-world graphs** có thể đạt tới hơn 200k **nodes**, khiến cho các **implementations** dựa trên **adjacency matrix** bị thất bại. Có rất nhiều **optimizations** khả thi khi **implementing GNNs**, và may mắn thay, tồn tại các **packages** cung cấp các **layers** như vậy.\n",
        "\n",
        "Các **packages** phổ biến nhất cho **PyTorch** là [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) và [Deep Graph Library](https://www.dgl.ai/) (cái sau thực ra là **framework agnostic**). Việc sử dụng cái nào phụ thuộc vào **project** bạn đang lên kế hoạch làm và sở thích cá nhân.\n",
        "\n",
        "Trong **tutorial** này, chúng ta sẽ xem xét **PyTorch Geometric** như là một phần của gia đình **PyTorch**. Tương tự như **PyTorch Lightning**, **PyTorch Geometric** không được cài đặt mặc định trên **Google Colab** (và thực ra cũng không có trong môi trường `dl2021` của chúng ta do có quá nhiều **dependencies** không cần thiết cho các bài thực hành). Do đó, hãy **import** và/hoặc **install** nó bên dưới:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIPEZprWJSZA",
        "outputId": "574f87e2-7c40-43d6-fe82-7328ff38d8fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RDKit WARNING: [19:12:50] Enabling RDKit 2019.09.3 jupyter extensions\n"
          ]
        }
      ],
      "source": [
        "# torch geometric\n",
        "try:\n",
        "    import torch_geometric\n",
        "except ModuleNotFoundError:\n",
        "    # Cài đặt các packages torch geometric với CUDA+PyTorch version cụ thể.\n",
        "    # Xem https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html để biết thêm chi tiết\n",
        "    TORCH = torch.__version__.split('+')[0]\n",
        "    CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "\n",
        "    # Cài đặt các thư viện phụ thuộc (dependencies) quan trọng\n",
        "    # -f chỉ định đường dẫn tìm file cài đặt (wheels) tương thích\n",
        "    !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-geometric\n",
        "\n",
        "    import torch_geometric\n",
        "\n",
        "# Import các module con thường dùng\n",
        "import torch_geometric.nn as geom_nn\n",
        "import torch_geometric.data as geom_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7BqFX6mJSZA"
      },
      "source": [
        "**PyTorch Geometric** cung cấp cho chúng ta một tập hợp các **graph layers** phổ biến, bao gồm lớp **GCN** và **GAT** mà chúng ta đã **implemented** ở trên.\n",
        "\n",
        "Ngoài ra, tương tự như **torchvision** của PyTorch, nó cung cấp các **common graph datasets** và các **transformations** trên chúng để đơn giản hóa quá trình **training**. So với **implementation** của chúng ta ở trên, **PyTorch Geometric** sử dụng một danh sách các **index pairs** (cặp chỉ số) để biểu diễn các **edges**. Chi tiết về thư viện này sẽ được khám phá sâu hơn trong các **experiments** của chúng ta.\n",
        "\n",
        "Trong các **tasks** bên dưới, chúng ta muốn cho phép mình chọn từ vô số các **graph layers**. Vì vậy, chúng ta định nghĩa lại bên dưới một **dictionary** để truy cập chúng bằng một **string**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_btxXFsJSZA"
      },
      "outputs": [],
      "source": [
        "gnn_layer_by_name = {\n",
        "    \"GCN\": geom_nn.GCNConv,\n",
        "    \"GAT\": geom_nn.GATConv,\n",
        "    \"GraphConv\": geom_nn.GraphConv\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a7xz2PdJSZB"
      },
      "source": [
        "Ngoài **GCN** và **GAT**, chúng ta đã thêm vào lớp `geom_nn.GraphConv` ([tài liệu](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GraphConv)).\n",
        "\n",
        "**GraphConv** là một **GCN** với một **weight matrix** riêng biệt cho các **self-connections**. Về mặt toán học, điều này sẽ là:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_i^{(l+1)} = \\mathbf{W}^{(l + 1)}_1 \\mathbf{x}_i^{(l)} + \\mathbf{W}^{(\\ell + 1)}_2 \\sum_{j \\in \\mathcal{N}_i} \\mathbf{x}_j^{(l)}\n",
        "$$\n",
        "\n",
        "Trong công thức này, các **messages** của **neighbor** được cộng lại (**added**) thay vì lấy trung bình (**averaged**). Tuy nhiên, **PyTorch Geometric** cung cấp tham số `aggr` để chuyển đổi giữa **summing**, **averaging**, và **max pooling**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXRQgsx-JSZB"
      },
      "source": [
        "## Experiments on graph structures (Thực nghiệm trên cấu trúc đồ thị)\n",
        "\n",
        "Các tác vụ trên dữ liệu có cấu trúc graph (**graph-structured data**) có thể được chia thành ba nhóm: **node-level**, **edge-level** và **graph-level**.\n",
        "\n",
        "Các cấp độ khác nhau này mô tả việc chúng ta muốn thực hiện **classification/regression** ở cấp độ nào. Chúng ta sẽ thảo luận chi tiết hơn về cả ba loại này ở bên dưới."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txwWGa8BJSZB"
      },
      "source": [
        "### Node-level tasks: Semi-supervised node classification\n",
        "\n",
        "Các **Node-level tasks** có mục tiêu là phân loại các **nodes** trong một **graph**.\n",
        "\n",
        "Thông thường, chúng ta được cung cấp một **single, large graph** (một đồ thị đơn lớn) với hơn 1000 **nodes**, trong đó một lượng nhất định các **nodes** đã được gán nhãn (**labeled**). Chúng ta học cách phân loại các ví dụ **labeled** đó trong quá trình **training** và cố gắng **generalize** (khái quát hóa) sang các **unlabeled nodes** (các node chưa có nhãn).\n",
        "\n",
        "Một ví dụ phổ biến mà chúng ta sẽ sử dụng trong **tutorial** này là **Cora dataset**, một mạng lưới trích dẫn (**citation network**) giữa các bài báo khoa học.\n",
        "\n",
        "[Image of citation network graph visualization]\n",
        "\n",
        "**Cora** bao gồm 2708 ấn phẩm khoa học với các liên kết giữa chúng đại diện cho việc trích dẫn của bài báo này đối với bài báo khác. **Task** ở đây là phân loại mỗi ấn phẩm vào một trong bảy **classes**.\n",
        "\n",
        "Mỗi ấn phẩm được đại diện bởi một **bag-of-words vector**. Điều này có nghĩa là chúng ta có một **vector** gồm 1433 phần tử cho mỗi ấn phẩm, trong đó số 1 tại **feature** $i$ chỉ ra rằng từ thứ $i$ của một từ điển xác định trước có xuất hiện trong bài báo đó.\n",
        "\n",
        "Các biểu diễn **Binary bag-of-words** thường được sử dụng khi chúng ta cần các **encodings** rất đơn giản, và đã có sẵn trực giác về những từ ngữ nào sẽ xuất hiện trong một mạng lưới. Có nhiều phương pháp tiếp cận tốt hơn nhiều, nhưng chúng ta sẽ để dành phần này cho các khóa học **NLP** thảo luận.\n",
        "\n",
        "Chúng ta sẽ tải **dataset** bên dưới:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfQcTpgtJSZB"
      },
      "outputs": [],
      "source": [
        "cora_dataset = torch_geometric.datasets.Planetoid(root=DATASET_PATH, name=\"Cora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ArHwGqTJSZB"
      },
      "source": [
        "Hãy xem xét cách **PyTorch Geometric** biểu diễn **graph data**.\n",
        "\n",
        "Lưu ý rằng mặc dù chúng ta có một **single graph** (đồ thị đơn), **PyTorch Geometric** vẫn trả về một **dataset** để đảm bảo tính tương thích (**compatibility**) với các **datasets** khác."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O3owwTX7Oq5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoKiinOKJSZC",
        "outputId": "48d4859b-eb6b-498a-d28f-6db003463f89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cora_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Giải thích chi tiết các thuộc tính:\n",
        "x=[2708, 1433] (Node Features)\n",
        "\n",
        "Ý nghĩa: Ma trận đặc trưng của các node.\n",
        "\n",
        "2708: Số lượng nodes (tương ứng với 2708 bài báo khoa học trong dataset).\n",
        "\n",
        "1433: Số lượng features của mỗi node (kích thước vector Bag-of-words). Mỗi bài báo được mô tả bởi sự xuất hiện của 1433 từ khóa.\n",
        "\n",
        "edge_index=[2, 10556] (Graph Connectivity)\n",
        "\n",
        "Ý nghĩa: Cấu trúc liên kết của đồ thị (các cạnh).\n",
        "\n",
        "10556: Tổng số lượng edges (các lượt trích dẫn) trong đồ thị.\n",
        "\n",
        "2: Tại sao lại là 2? PyTorch Geometric lưu trữ cạnh dưới dạng COO format (Coordinate Format).\n",
        "\n",
        "Hàng 1: Index của node nguồn (Source nodes).\n",
        "\n",
        "Hàng 2: Index của node đích (Target nodes).\n",
        "\n",
        "Ví dụ: Một cột [0, 5] nghĩa là có cạnh nối từ Node 0 đến Node 5.\n",
        "\n",
        "y=[2708] (Labels)\n",
        "\n",
        "Ý nghĩa: Nhãn phân loại của từng node (Ground truth).\n",
        "\n",
        "2708: Mỗi bài báo sẽ có 1 nhãn tương ứng (từ 0 đến 6, đại diện cho 7 chủ đề khoa học khác nhau).\n",
        "\n",
        "train_mask=[2708], val_mask=[2708], test_mask=[2708]\n",
        "\n",
        "Ý nghĩa: Các mặt nạ (Masks) dùng để chia dữ liệu.\n",
        "\n",
        "Đây là các mảng Boolean (True hoặc False).\n",
        "\n",
        "Vì chúng ta chỉ có 1 Graph duy nhất, chúng ta không thể chia graph ra làm 3 phần rời nhau (sẽ mất kết nối). Thay vào đó, ta dùng mask để đánh dấu:\n",
        "\n",
        "train_mask: Node nào được dùng để tính loss và cập nhật trọng số (Training).\n",
        "\n",
        "val_mask: Node nào dùng để tinh chỉnh mô hình (Validation).\n",
        "\n",
        "test_mask: Node nào dùng để đánh giá cuối cùng (Testing).\n",
        "\n",
        "Kích thước 2708 nghĩa là mỗi node đều có một trạng thái True/False tương ứng trong từng mask."
      ],
      "metadata": {
        "id": "AOZuFoqPOc91"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zmt9f3iJSZC"
      },
      "source": [
        "**Graph** được biểu diễn bởi một `Data` object ([tài liệu](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data)) mà chúng ta có thể truy cập như một **standard Python namespace**.\n",
        "\n",
        "**Edge index tensor** là danh sách các **edges** trong **graph** và chứa phiên bản đối xứng (**mirrored version**) của mỗi **edge** đối với các **undirected graphs**.\n",
        "\n",
        "Các `train_mask`, `val_mask`, và `test_mask` là các **boolean masks** chỉ ra những **nodes** nào chúng ta nên sử dụng cho **training**, **validation**, và **testing**.\n",
        "\n",
        "`x` tensor là **feature tensor** của 2708 ấn phẩm (**publications**) của chúng ta, và `y` là các **labels** cho tất cả các **nodes**.\n",
        "\n",
        "Sau khi đã xem qua dữ liệu, chúng ta có thể **implement** một **graph neural network** đơn giản. **GNN** áp dụng một chuỗi các **graph layers** (GCN, GAT, hoặc GraphConv), **ReLU** làm **activation function**, và **dropout** để **regularization**. Xem bên dưới để biết **implementation** cụ thể."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boLy7D5EJSZC"
      },
      "outputs": [],
      "source": [
        "class GNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GCN\", dp_rate=0.1, **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimension của input features\n",
        "            c_hidden - Dimension của hidden features\n",
        "            c_out - Dimension của output features. Thường là số lượng classes trong classification\n",
        "            num_layers - Số lượng các graph layers \"ẩn\" (hidden)\n",
        "            layer_name - String tên của graph layer cần sử dụng\n",
        "            dp_rate - Dropout rate được áp dụng xuyên suốt network\n",
        "            kwargs - Các tham số bổ sung cho graph layer (ví dụ: số lượng heads cho GAT)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Lấy lớp layer tương ứng từ dictionary (đã định nghĩa trước đó) dựa trên tên\n",
        "        gnn_layer = gnn_layer_by_name[layer_name]\n",
        "\n",
        "        layers = []\n",
        "        in_channels, out_channels = c_in, c_hidden\n",
        "\n",
        "        # Vòng lặp tạo các lớp ẩn (Hidden Layers)\n",
        "        for l_idx in range(num_layers-1):\n",
        "            layers += [\n",
        "                gnn_layer(in_channels=in_channels,\n",
        "                          out_channels=out_channels,\n",
        "                          **kwargs),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dp_rate)\n",
        "            ]\n",
        "            in_channels = c_hidden # Cập nhật input channels cho lớp tiếp theo\n",
        "\n",
        "        # Thêm lớp output cuối cùng (không có ReLU/Dropout sau lớp này để lấy logits)\n",
        "        layers += [gnn_layer(in_channels=in_channels,\n",
        "                             out_channels=c_out,\n",
        "                             **kwargs)]\n",
        "\n",
        "        # Đóng gói danh sách layers vào nn.ModuleList để PyTorch quản lý tham số\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features cho mỗi node\n",
        "            edge_index - List các cặp chỉ số vertex biểu diễn các edges trong graph (ký hiệu của PyTorch Geometric)\n",
        "        \"\"\"\n",
        "        for l in self.layers:\n",
        "            # Đối với các graph layers, chúng ta cần thêm tensor \"edge_index\" như là input bổ sung\n",
        "            # Tất cả các PyTorch Geometric graph layer đều thừa kế class \"MessagePassing\", do đó\n",
        "            # chúng ta có thể đơn giản là kiểm tra loại class (class type).\n",
        "            if isinstance(l, geom_nn.MessagePassing):\n",
        "                x = l(x, edge_index)\n",
        "            else:\n",
        "                # Các lớp như ReLU hay Dropout chỉ cần input x\n",
        "                x = l(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d1KZ7KSJSZC"
      },
      "source": [
        "Một thực hành tốt trong các **node-level tasks** là tạo ra một **MLP baseline** được áp dụng cho mỗi **node** một cách độc lập.\n",
        "\n",
        "Theo cách này, chúng ta có thể xác minh xem liệu việc thêm **graph information** vào **model** thực sự có cải thiện **prediction** hay không, hay là không cần thiết.\n",
        "\n",
        "Cũng có thể xảy ra trường hợp là các **features** trên mỗi **node** đã đủ **expressive** (có tính biểu đạt) để chỉ rõ về một **class** cụ thể mà không cần quan tâm đến hàng xóm. Để kiểm tra điều này, chúng ta **implement** một **MLP** đơn giản bên dưới."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN3vG90LJSZD"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimension của input features\n",
        "            c_hidden - Dimension của hidden features\n",
        "            c_out - Dimension của output features. Thường là số lượng classes trong classification\n",
        "            num_layers - Số lượng hidden layers\n",
        "            dp_rate - Dropout rate được áp dụng xuyên suốt network\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        in_channels, out_channels = c_in, c_hidden\n",
        "\n",
        "        # Vòng lặp tạo các lớp ẩn (Hidden Layers)\n",
        "        for l_idx in range(num_layers-1):\n",
        "            layers += [\n",
        "                nn.Linear(in_channels, out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dp_rate)\n",
        "            ]\n",
        "            in_channels = c_hidden\n",
        "\n",
        "        # Thêm lớp Linear cuối cùng để ra output\n",
        "        layers += [nn.Linear(in_channels, c_out)]\n",
        "\n",
        "        # Sử dụng nn.Sequential vì luồng dữ liệu là tuần tự, không cần xử lý edge_index phức tạp\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features cho mỗi node\n",
        "        \"\"\"\n",
        "        # *args, **kwargs được thêm vào để tương thích với cấu trúc gọi hàm của GNN\n",
        "        # GNN sẽ truyền vào (x, edge_index), nhưng MLP chỉ cần x và lờ đi edge_index\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3ucTEiOJSZD"
      },
      "source": [
        "Cuối cùng, chúng ta có thể hợp nhất các **models** vào một **PyTorch Lightning module**, module này sẽ xử lý việc **training**, **validation**, và **testing** cho chúng ta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMTFyi1RJSZD"
      },
      "outputs": [],
      "source": [
        "class NodeLevelGNN(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model_name, **model_kwargs):\n",
        "        super().__init__()\n",
        "        # Lưu các hyperparameters (tham số siêu hình) để tiện cho việc checkpoint và log\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Chọn kiến trúc model dựa trên tên\n",
        "        if model_name == \"MLP\":\n",
        "            self.model = MLPModel(**model_kwargs)\n",
        "        else:\n",
        "            self.model = GNNModel(**model_kwargs)\n",
        "\n",
        "        # Hàm loss function cho bài toán phân loại nhiều lớp (Multi-class classification)\n",
        "        self.loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, data, mode=\"train\"):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # Forward pass qua model (GNN hoặc MLP)\n",
        "        x = self.model(x, edge_index)\n",
        "\n",
        "        # Chỉ tính toán loss trên các nodes tương ứng với mask\n",
        "        # Vì đây là Semi-supervised learning trên 1 graph duy nhất\n",
        "        if mode == \"train\":\n",
        "            mask = data.train_mask\n",
        "        elif mode == \"val\":\n",
        "            mask = data.val_mask\n",
        "        elif mode == \"test\":\n",
        "            mask = data.test_mask\n",
        "        else:\n",
        "            assert False, f\"Unknown forward mode: {mode}\"\n",
        "\n",
        "        # Tính Loss chỉ trên các node được phép nhìn thấy (theo mask)\n",
        "        loss = self.loss_module(x[mask], data.y[mask])\n",
        "\n",
        "        # Tính Accuracy\n",
        "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
        "        return loss, acc\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Chúng ta dùng SGD ở đây, nhưng Adam cũng hoạt động tốt\n",
        "        # Weight decay giúp giảm overfitting\n",
        "        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, acc = self.forward(batch, mode=\"train\")\n",
        "        # Log kết quả để theo dõi\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"val\")\n",
        "        self.log('val_acc', acc)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"test\")\n",
        "        self.log('test_acc', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6zLbm8BJSZE"
      },
      "source": [
        "Ngoài **Lightning module**, chúng ta định nghĩa một **training function** bên dưới.\n",
        "\n",
        "Vì chúng ta có một **single graph** (đồ thị đơn), chúng ta sử dụng **batch size** là 1 cho **data loader** và chia sẻ cùng một **data loader** cho **train**, **validation**, và **test set** (**mask** được chọn bên trong **Lightning module**).\n",
        "\n",
        "Bên cạnh đó, chúng ta thiết lập tham số `enable_progress_bar` thành False vì nó thường hiển thị **progress** (tiến độ) trên mỗi **epoch**, nhưng một **epoch** ở đây chỉ bao gồm một **single step**. Phần còn lại của code rất giống với những gì chúng ta đã thấy trong Tutorial 5 và 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTs5IIFcJSZE"
      },
      "outputs": [],
      "source": [
        "def train_node_classifier(model_name, dataset, **model_kwargs):\n",
        "    pl.seed_everything(42)\n",
        "    # Tạo DataLoader với batch_size=1 vì chúng ta chỉ có 1 Graph duy nhất\n",
        "    node_data_loader = geom_data.DataLoader(dataset, batch_size=1)\n",
        "\n",
        "    # Tạo một PyTorch Lightning trainer với callback để lưu model\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"NodeLevel\" + model_name)\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
        "                         # ModelCheckpoint: Tự động lưu lại trọng số của model tốt nhất dựa trên val_acc\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         # Tự động chọn GPU nếu có, ngược lại dùng CPU\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=200,\n",
        "                         enable_progress_bar=False) # False vì epoch size là 1 (quá nhanh để hiển thị thanh tiến độ)\n",
        "\n",
        "    trainer.logger._default_hp_metric = None # Tham số logging tùy chọn mà chúng ta không cần\n",
        "\n",
        "    # Kiểm tra xem pretrained model đã tồn tại chưa. Nếu có, load nó và bỏ qua training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"NodeLevel{model_name}.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Tìm thấy pretrained model, đang loading...\")\n",
        "        model = NodeLevelGNN.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything()\n",
        "        # Khởi tạo model mới: NodeLevelGNN\n",
        "        model = NodeLevelGNN(model_name=model_name, c_in=dataset.num_node_features, c_out=dataset.num_classes, **model_kwargs)\n",
        "        # Bắt đầu huấn luyện (dùng validation set trùng với train set trong loader, nhưng logic mask sẽ xử lý việc tách biệt)\n",
        "        trainer.fit(model, node_data_loader, node_data_loader)\n",
        "        # Load lại model tốt nhất sau khi train xong\n",
        "        model = NodeLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "    # Test model tốt nhất trên test set\n",
        "    test_result = trainer.test(model, node_data_loader, verbose=False)\n",
        "\n",
        "    # Tính toán thêm độ chính xác trên tập Train và Val để báo cáo đầy đủ\n",
        "    batch = next(iter(node_data_loader))\n",
        "    batch = batch.to(model.device)\n",
        "    _, train_acc = model.forward(batch, mode=\"train\")\n",
        "    _, val_acc = model.forward(batch, mode=\"val\")\n",
        "\n",
        "    result = {\"train\": train_acc,\n",
        "              \"val\": val_acc,\n",
        "              \"test\": test_result[0]['test_acc']}\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDgt8ngmJSZE"
      },
      "source": [
        "Cuối cùng, chúng ta có thể huấn luyện các mô hình của mình. Trước tiên, hãy huấn luyện MLP đơn giản:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAWjrmKGJSZK"
      },
      "outputs": [],
      "source": [
        "# Hàm nhỏ để in ra các điểm số kiểm tra (test scores)\n",
        "def print_results(result_dict):\n",
        "    if \"train\" in result_dict:\n",
        "        # In ra độ chính xác tập Train, định dạng 2 chữ số thập phân\n",
        "        print(f\"Train accuracy: {(100.0*result_dict['train']):4.2f}%\")\n",
        "    if \"val\" in result_dict:\n",
        "        # In ra độ chính xác tập Validation\n",
        "        print(f\"Val accuracy:   {(100.0*result_dict['val']):4.2f}%\")\n",
        "    # In ra độ chính xác tập Test\n",
        "    print(f\"Test accuracy:  {(100.0*result_dict['test']):4.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naPFHGtoJSZK",
        "outputId": "efe27359-0a34-48ec-e547-5fa44554a9ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "I1113 19:12:52.401648 139969460983616 distributed.py:49] GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "I1113 19:12:52.403518 139969460983616 distributed.py:49] TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "I1113 19:12:52.404974 139969460983616 accelerator_connector.py:385] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found pretrained model, loading...\n",
            "Train accuracy: 96.43%\n",
            "Val accuracy:   52.60%\n",
            "Test accuracy:  60.60%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/phillip/anaconda3/envs/nlp1/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "node_mlp_model, node_mlp_result = train_node_classifier(model_name=\"MLP\",\n",
        "                                                        dataset=cora_dataset,\n",
        "                                                        c_hidden=16,\n",
        "                                                        num_layers=2,\n",
        "                                                        dp_rate=0.1)\n",
        "\n",
        "print_results(node_mlp_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77xgsequJSZK"
      },
      "source": [
        "Mặc dù **MLP** có thể bị **overfit** trên **training dataset** do các **high-dimensional input features**, nó không hoạt động quá tốt trên **test set**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bây giờ chúng ta sẽ chạy code để huấn luyện lần lượt: MLP (để lấy điểm chuẩn) và sau đó là GCN (để xem có cải thiện không)."
      ],
      "metadata": {
        "id": "XViK4ruIP6d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Huấn luyện mô hình MLP (Baseline)\n",
        "node_mlp_model, node_mlp_result = train_node_classifier(model_name=\"MLP\",\n",
        "                                                        dataset=cora_dataset,\n",
        "                                                        c_hidden=16,\n",
        "                                                        num_layers=2,\n",
        "                                                        dp_rate=0.1)\n",
        "\n",
        "# In kết quả\n",
        "print_results(node_mlp_result)"
      ],
      "metadata": {
        "id": "OwMbfODJP9C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hãy xem liệu chúng ta có thể đánh bại điểm số này với các **graph networks** của chúng ta hay không:"
      ],
      "metadata": {
        "id": "RxvhkIQPQCSN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq42hO70JSZL",
        "outputId": "0cf3e537-c13e-4814-b609-fefe0b328242"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "I1113 19:12:53.906714 139969460983616 distributed.py:49] GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "I1113 19:12:53.907762 139969460983616 distributed.py:49] TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "I1113 19:12:53.909639 139969460983616 accelerator_connector.py:385] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found pretrained model, loading...\n",
            "Train accuracy: 100.00%\n",
            "Val accuracy:   77.20%\n",
            "Test accuracy:  82.40%\n"
          ]
        }
      ],
      "source": [
        "node_gnn_model, node_gnn_result = train_node_classifier(model_name=\"GNN\",\n",
        "                                                        layer_name=\"GCN\",\n",
        "                                                        dataset=cora_dataset,\n",
        "                                                        c_hidden=16,\n",
        "                                                        num_layers=2,\n",
        "                                                        dp_rate=0.1)\n",
        "print_results(node_gnn_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AdGPivWJSZL"
      },
      "source": [
        "Như chúng ta đã kỳ vọng, **GNN model** vượt trội hơn **MLP** với một khoảng cách khá lớn. Điều này cho thấy việc sử dụng **graph information** thực sự cải thiện các **predictions** của chúng ta và giúp chúng ta **generalizes** (khái quát hóa) tốt hơn.\n",
        "\n",
        "Các **hyperparameters** trong **model** đã được chọn để tạo ra một mạng tương đối nhỏ. Điều này là do lớp đầu tiên với **input dimension** là 1433 có thể tương đối tốn kém (về mặt tính toán) để thực hiện đối với các **large graphs**.\n",
        "\n",
        "Nhìn chung, **GNNs** có thể trở nên tương đối tốn kém đối với các **very big graphs**. Đây là lý do tại sao các **GNNs** như vậy hoặc là có **hidden size** nhỏ, hoặc sử dụng một **batching strategy** đặc biệt nơi chúng ta **sample** (lấy mẫu) một **connected subgraph** (đồ thị con có kết nối) của **graph** lớn ban đầu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G12itspZJSZL"
      },
      "source": [
        "### Edge-level tasks: Link prediction\n",
        "\n",
        "Trong một số ứng dụng, chúng ta có thể phải dự đoán ở **edge-level** thay vì **node-level**. **Edge-level task** phổ biến nhất trong **GNN** là **link prediction**.\n",
        "\n",
        "**Link prediction** có nghĩa là với một **graph** cho trước, chúng ta muốn dự đoán liệu sẽ có/nên có một **edge** giữa hai **nodes** hay không. Ví dụ, trong một **social network** (mạng xã hội), điều này được Facebook và các công ty tương tự sử dụng để đề xuất bạn mới cho bạn. Một lần nữa, **graph level information** có thể rất quan trọng để thực hiện **task** này.\n",
        "\n",
        "**Output prediction** thường được thực hiện bằng cách tính toán một **similarity metric** (độ đo tương đồng) trên cặp **node features**, giá trị này nên là 1 nếu nên có một **link**, và ngược lại gần bằng 0.\n",
        "\n",
        "Để giữ cho **tutorial** ngắn gọn, chúng ta sẽ không tự **implement** **task** này. Tuy nhiên, có rất nhiều tài nguyên tốt ngoài kia nếu bạn quan tâm đến việc xem xét kỹ hơn **task** này.\n",
        "\n",
        "Các hướng dẫn và bài báo cho chủ đề này bao gồm:\n",
        "\n",
        "* [PyTorch Geometric example](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/link_pred.py)\n",
        "* [Graph Neural Networks: A Review of Methods and Applications](https://arxiv.org/pdf/1812.08434.pdf), Zhou et al. 2019\n",
        "* [Link Prediction Based on Graph Neural Networks](https://papers.nips.cc/paper/2018/file/53f0d7c537d99b3824f0f99d62ea2428-Paper.pdf), Zhang and Chen, 2018."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr_chuzZJSZL"
      },
      "source": [
        "### Graph-level tasks: Graph classification\n",
        "\n",
        "Cuối cùng, trong phần này của **tutorial**, chúng ta sẽ xem xét kỹ hơn cách áp dụng **GNNs** vào **task** của **graph classification**.\n",
        "\n",
        "Mục tiêu là phân loại một **entire graph** (toàn bộ đồ thị) thay vì các **single nodes** hay **edges**. Do đó, chúng ta cũng được cung cấp một **dataset** gồm **multiple graphs** mà chúng ta cần phân loại dựa trên một số **structural graph properties** (thuộc tính cấu trúc đồ thị).\n",
        "\n",
        "**Task** phổ biến nhất cho **graph classification** là **molecular property prediction** (dự đoán tính chất phân tử), trong đó các **molecules** (phân tử) được biểu diễn dưới dạng **graphs**. Mỗi **atom** (nguyên tử) được liên kết với một **node**, và các **edges** trong **graph** là các **bonds** (liên kết hóa học) giữa các **atoms**. Ví dụ, hãy nhìn vào hình bên dưới.\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/molecule_graph.svg?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "Ở bên trái, chúng ta có một **molecule** nhỏ, bất kỳ với các **atoms** khác nhau, trong khi phần bên phải của hình ảnh hiển thị **graph representation**.\n",
        "\n",
        "Các **atom types** được trừu tượng hóa thành **node features** (ví dụ: một **one-hot vector**), và các **bond types** khác nhau được sử dụng làm **edge features**. Để đơn giản, chúng ta sẽ bỏ qua các **edge attributes** trong **tutorial** này, nhưng bạn có thể đưa vào bằng cách sử dụng các phương pháp như [Relational Graph Convolution](https://arxiv.org/abs/1703.06103) cái mà sử dụng một **weight matrix** khác nhau cho mỗi **edge type**.\n",
        "\n",
        "**Dataset** chúng ta sẽ sử dụng bên dưới được gọi là **MUTAG dataset**. Nó là một **benchmark** nhỏ phổ biến cho các thuật toán **graph classification**, và chứa 188 **graphs** với trung bình 18 **nodes** và 20 **edges** cho mỗi **graph**.\n",
        "\n",
        "Các **graph nodes** có 7 **labels/atom types** khác nhau, và các **binary graph labels** đại diện cho \"hiệu ứng đột biến gen của chúng trên một loại vi khuẩn gram âm cụ thể\" (ý nghĩa cụ thể của các **labels** không quá quan trọng ở đây).\n",
        "\n",
        "**Dataset** này là một phần của bộ sưu tập lớn các **graph classification datasets** khác nhau, được biết đến là [TUDatasets](https://chrsmrrs.github.io/datasets/), có thể truy cập trực tiếp thông qua `torch_geometric.datasets.TUDataset` ([tài liệu](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.TUDataset)) trong **PyTorch Geometric**. Chúng ta có thể tải **dataset** bên dưới."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgdXtU_VJSZL"
      },
      "outputs": [],
      "source": [
        "tu_dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"MUTAG\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P33lDApdJSZM"
      },
      "source": [
        "Hãy xem một số thống kê về bộ dữ liệu:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG0wsnSlJSZM",
        "outputId": "4678becc-dfb6-4f0a-c336-31a3e2cc83ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data object: Data(edge_attr=[7442, 4], edge_index=[2, 7442], x=[3371, 7], y=[188])\n",
            "Length: 188\n",
            "Average label: 0.66\n"
          ]
        }
      ],
      "source": [
        "print(\"Data object:\", tu_dataset.data)\n",
        "print(\"Length:\", len(tu_dataset))\n",
        "print(f\"Average label: {tu_dataset.data.y.float().mean().item():4.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjdWwm_RJSZM"
      },
      "source": [
        "Dòng đầu tiên cho thấy cách **dataset** lưu trữ các **graphs** khác nhau.\n",
        "\n",
        "Các **nodes**, **edges**, và **labels** của mỗi **graph** được **concatenated** (nối lại) thành một **tensor** duy nhất, và **dataset** lưu trữ các chỉ số (**indices**) để chia tách các **tensors** một cách tương ứng.\n",
        "\n",
        "Độ dài của **dataset** là số lượng **graphs** chúng ta có, và \"**average label**\" biểu thị phần trăm của **graph** có **label** là 1. Miễn là phần trăm nằm trong khoảng 0.5, chúng ta có một **dataset** tương đối **balanced** (cân bằng). Việc các **graph datasets** bị **imbalanced** (mất cân bằng) xảy ra khá thường xuyên, do đó việc kiểm tra **class balance** luôn là một việc tốt nên làm.\n",
        "\n",
        "Tiếp theo, chúng ta sẽ chia **dataset** của mình thành phần **training** và **test**. Lưu ý rằng chúng ta không sử dụng **validation set** lần này do kích thước nhỏ của **dataset**.\n",
        "\n",
        "Do đó, **model** của chúng ta có thể bị **overfit** nhẹ (do không có validation để dừng sớm), nhưng chúng ta vẫn có được một ước lượng về hiệu suất trên dữ liệu chưa được huấn luyện (**untrained data**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHjBp0IoJSZM"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "tu_dataset.shuffle()\n",
        "train_dataset = tu_dataset[:150]\n",
        "test_dataset = tu_dataset[150:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoKKapuhJSZM"
      },
      "source": [
        "Khi sử dụng một **data loader**, chúng ta gặp phải một vấn đề với việc **batching** $N$ **graphs**.\n",
        "\n",
        "Mỗi **graph** trong **batch** có thể có số lượng **nodes** và **edges** khác nhau, và do đó chúng ta sẽ cần rất nhiều **padding** để thu được một **tensor** duy nhất.\n",
        "\n",
        "**Torch geometric** sử dụng một cách tiếp cận khác, hiệu quả hơn: chúng ta có thể xem $N$ **graphs** trong một **batch** như là một **single large graph** (một đồ thị lớn duy nhất) với **concatenated node and edge list** (danh sách nút và cạnh được nối lại với nhau).\n",
        "\n",
        "Vì không có **edge** nào giữa $N$ **graphs**, việc chạy các **GNN layers** trên **large graph** mang lại cho chúng ta **output** giống y hệt như việc chạy **GNN** trên từng **graph** riêng biệt. Về mặt hình ảnh, chiến lược **batching** này được trực quan hóa bên dưới (nguồn hình ảnh - nhóm PyTorch Geometric, [hướng dẫn tại đây](https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=2owRWKcuoALo)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/torch_geometric_stacking_graphs.png?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "**Adjacency matrix** sẽ bằng 0 đối với bất kỳ **nodes** nào đến từ hai **graphs** khác nhau, và ngược lại thì tuân theo **adjacency matrix** của **individual graph** (đồ thị cá nhân).\n",
        "\n",
        "May mắn thay, chiến lược này đã được **implemented** sẵn trong **torch geometric**, và do đó chúng ta có thể sử dụng **data loader** tương ứng:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkAck7KrJSZM"
      },
      "outputs": [],
      "source": [
        "# Tạo DataLoader cho tập huấn luyện (Training Set)\n",
        "# batch_size=64: Mỗi lần đưa vào model 64 đồ thị phân tử cùng lúc\n",
        "# shuffle=True: Xáo trộn dữ liệu mỗi epoch để model học tốt hơn, không bị học vẹt theo thứ tự\n",
        "graph_train_loader = geom_data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Tạo DataLoader cho tập Validation (Sử dụng tạm test_dataset vì dữ liệu MUTAG quá nhỏ)\n",
        "graph_val_loader = geom_data.DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# Tạo DataLoader cho tập Kiểm thử (Test Set)\n",
        "graph_test_loader = geom_data.DataLoader(test_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjecZJegJSZN"
      },
      "source": [
        "Hãy tải một batch bên dưới để xem cách hoạt động của việc batching:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlA0ewl2JSZN",
        "outputId": "a5ba7b72-3fd6-4215-a7ec-88f53948e4a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch: Batch(batch=[687], edge_attr=[1512, 4], edge_index=[2, 1512], x=[687, 7], y=[38])\n",
            "Labels: tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0])\n",
            "Batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "# Lấy ra batch đầu tiên từ test loader để kiểm tra\n",
        "batch = next(iter(graph_test_loader))\n",
        "\n",
        "print(\"Batch:\", batch)\n",
        "# In thử 10 nhãn đầu tiên (tương ứng với 10 đồ thị đầu trong batch)\n",
        "print(\"Labels:\", batch.y[:10])\n",
        "# In thử 40 chỉ số batch đầu tiên (để xem các node thuộc về đồ thị nào)\n",
        "print(\"Batch indices:\", batch.batch[:40])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk95LFf8JSZN"
      },
      "source": [
        "Chúng ta có 38 **graphs** được xếp chồng lên nhau (**stacked together**) cho **test dataset**.\n",
        "\n",
        "Các **batch indices**, được lưu trong `batch`, chỉ ra rằng 12 **nodes** đầu tiên thuộc về **graph** thứ nhất, 22 **nodes** tiếp theo thuộc về **graph** thứ hai, và cứ thế tiếp tục. Các **indices** này rất quan trọng để thực hiện **final prediction** (dự đoán cuối cùng).\n",
        "\n",
        "Để thực hiện một **prediction** trên toàn bộ một **graph**, chúng ta thường thực hiện một **pooling operation** trên tất cả các **nodes** sau khi chạy **GNN model**. Trong trường hợp này, chúng ta sẽ sử dụng **average pooling** (gom nhóm trung bình).\n",
        "\n",
        "Do đó, chúng ta cần biết những **nodes** nào nên được bao gồm trong **average pool** nào. Sử dụng **pooling** này, chúng ta đã có thể tạo **graph network** của mình bên dưới. Cụ thể, chúng ta tái sử dụng **class** `GNNModel` từ trước, và đơn giản là thêm một **average pool** và một **single linear layer** cho **graph prediction task**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8n35muWJSZN"
      },
      "outputs": [],
      "source": [
        "class GraphGNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimension của input features\n",
        "            c_hidden - Dimension của hidden features\n",
        "            c_out - Dimension của output features (thường là số lượng classes)\n",
        "            dp_rate_linear - Dropout rate trước lớp linear (thường cao hơn nhiều so với bên trong GNN)\n",
        "            kwargs - Các tham số bổ sung cho đối tượng GNNModel\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Khởi tạo phần GNN (Backbone) để trích xuất đặc trưng cho từng Node\n",
        "        # Lưu ý: c_out của GNN lúc này là c_hidden, chưa phải là output cuối cùng!\n",
        "        self.GNN = GNNModel(c_in=c_in,\n",
        "                            c_hidden=c_hidden,\n",
        "                            c_out=c_hidden,\n",
        "                            **kwargs)\n",
        "\n",
        "        # Phần \"Head\" (Đầu ra) dùng để phân loại sau khi đã Pooling\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dp_rate_linear),\n",
        "            nn.Linear(c_hidden, c_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, batch_idx):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features cho mỗi node\n",
        "            edge_index - List các cặp chỉ số vertex biểu diễn edges (ký hiệu của PyTorch Geometric)\n",
        "            batch_idx - Index của phần tử batch cho mỗi node (cho biết node thuộc về graph nào)\n",
        "        \"\"\"\n",
        "        # Bước 1: Trích xuất đặc trưng cho từng node\n",
        "        # Output: [Total Nodes, c_hidden]\n",
        "        x = self.GNN(x, edge_index)\n",
        "\n",
        "        # Bước 2: Pooling (Gom nhóm)\n",
        "        # Biến đổi từ Node Embeddings -> Graph Embeddings\n",
        "        # Lấy trung bình cộng các node thuộc cùng 1 graph (dựa vào batch_idx)\n",
        "        # Output: [Batch Size, c_hidden]\n",
        "        x = geom_nn.global_mean_pool(x, batch_idx)\n",
        "\n",
        "        # Bước 3: Phân loại (Prediction)\n",
        "        # Output: [Batch Size, c_out]\n",
        "        x = self.head(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRpg-inMJSZO"
      },
      "source": [
        "Cuối cùng, chúng ta có thể tạo một **PyTorch Lightning module** để xử lý việc **training**.\n",
        "\n",
        "Nó tương tự như các **modules** chúng ta đã thấy trước đây và không làm gì đáng ngạc nhiên về mặt **training**. Vì chúng ta có một **binary classification task** (bài toán phân loại nhị phân), chúng ta sử dụng **Binary Cross Entropy loss**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGb8QQo0JSZO"
      },
      "outputs": [],
      "source": [
        "class GraphLevelGNN(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, **model_kwargs):\n",
        "        super().__init__()\n",
        "        # Lưu hyperparameters\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Khởi tạo model GraphGNNModel (đã định nghĩa ở bước trước)\n",
        "        self.model = GraphGNNModel(**model_kwargs)\n",
        "\n",
        "        # Chọn hàm Loss function:\n",
        "        # Nếu c_out (số lớp đầu ra) là 1 -> Bài toán Binary Classification -> Dùng BCEWithLogitsLoss\n",
        "        # Nếu c_out > 1 -> Bài toán Multi-class Classification -> Dùng CrossEntropyLoss\n",
        "        self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, data, mode=\"train\"):\n",
        "        # Lấy dữ liệu từ batch\n",
        "        # data.batch chính là batch_idx (vector chỉ định node nào thuộc graph nào)\n",
        "        x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Forward pass qua model\n",
        "        x = self.model(x, edge_index, batch_idx)\n",
        "\n",
        "        # Loại bỏ chiều cuối nếu kích thước là [Batch Size, 1] -> [Batch Size]\n",
        "        x = x.squeeze(dim=-1)\n",
        "\n",
        "        # Tính toán dự đoán (Predictions)\n",
        "        if self.hparams.c_out == 1:\n",
        "            # Đối với Binary: Giá trị > 0 tương ứng với xác suất sigmoid > 0.5 -> Class 1\n",
        "            preds = (x > 0).float()\n",
        "            # Chuyển label sang dạng float để tính BCE Loss\n",
        "            data.y = data.y.float()\n",
        "        else:\n",
        "            # Đối với Multi-class: Lấy chỉ số có giá trị lớn nhất\n",
        "            preds = x.argmax(dim=-1)\n",
        "\n",
        "        # Tính Loss và Accuracy\n",
        "        loss = self.loss_module(x, data.y)\n",
        "        acc = (preds == data.y).sum().float() / preds.shape[0]\n",
        "        return loss, acc\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Sử dụng AdamW optimizer\n",
        "        # Learning rate cao (1e-2) vì dataset nhỏ và model nhỏ\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=1e-2, weight_decay=0.0)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, acc = self.forward(batch, mode=\"train\")\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"val\")\n",
        "        self.log('val_acc', acc)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"test\")\n",
        "        self.log('test_acc', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvfSo0lbJSZO"
      },
      "source": [
        "Bên dưới, chúng ta huấn luyện **model** trên **dataset** của chúng ta. Nó giống với các **training functions** điển hình mà chúng ta đã thấy cho đến nay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQmN3XHVJSZO"
      },
      "outputs": [],
      "source": [
        "def train_graph_classifier(model_name, **model_kwargs):\n",
        "    pl.seed_everything(42)\n",
        "\n",
        "    # Tạo một PyTorch Lightning trainer với callback để lưu checkpoint\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
        "                         # ModelCheckpoint: Lưu lại trọng số tốt nhất dựa trên val_acc\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=500, # Train lâu hơn (500 epochs) vì dataset nhỏ và cần hội tụ kỹ\n",
        "                         enable_progress_bar=False)\n",
        "\n",
        "    trainer.logger._default_hp_metric = None # Tham số logging tùy chọn mà chúng ta không cần\n",
        "\n",
        "    # Kiểm tra xem pretrained model đã tồn tại chưa. Nếu có, load nó và bỏ qua training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"GraphLevel{model_name}.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Tìm thấy pretrained model, đang loading...\")\n",
        "        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything(42)\n",
        "        # Khởi tạo model GraphLevelGNN\n",
        "        # Logic c_out: Nếu bài toán có 2 class -> Output 1 node (Binary). Nếu > 2 class -> Output N node (Multi-class)\n",
        "        model = GraphLevelGNN(c_in=tu_dataset.num_node_features,\n",
        "                              c_out=1 if tu_dataset.num_classes==2 else tu_dataset.num_classes,\n",
        "                              **model_kwargs)\n",
        "\n",
        "        # Bắt đầu huấn luyện với train_loader và val_loader\n",
        "        trainer.fit(model, graph_train_loader, graph_val_loader)\n",
        "        # Load lại model tốt nhất sau khi train xong\n",
        "        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "    # Test model tốt nhất trên validation và test set\n",
        "    # Lưu ý: PyTorch Lightning gọi hàm test nhưng trả về dictionary kết quả, ta lấy 'test_acc' từ đó\n",
        "    train_result = trainer.test(model, graph_train_loader, verbose=False)\n",
        "    test_result = trainer.test(model, graph_test_loader, verbose=False)\n",
        "\n",
        "    result = {\"test\": test_result[0]['test_acc'], \"train\": train_result[0]['test_acc']}\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3ttOweeJSZP"
      },
      "source": [
        "Cuối cùng, hãy thực hiện việc **training** và **testing**. Hãy thoải mái thử nghiệm với các **GNN layers**, **hyperparameters** khác nhau, v.v."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "BidvPkvqJSZP",
        "outputId": "b906037f-3284-44d1-83f4-5fec92657da6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "I1113 19:12:54.045717 139969460983616 distributed.py:49] GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "I1113 19:12:54.047091 139969460983616 distributed.py:49] TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "I1113 19:12:54.048336 139969460983616 accelerator_connector.py:385] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/home/phillip/anaconda3/envs/nlp1/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Your test_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found pretrained model, loading...\n"
          ]
        }
      ],
      "source": [
        "# Huấn luyện mô hình\n",
        "# model_name: Tên dùng để lưu checkpoint\n",
        "# c_hidden=256: Số lượng features ẩn lớn hơn nhiều so với bài toán Node-level (chỉ 16)\n",
        "# layer_name=\"GraphConv\": Sử dụng lớp GraphConv\n",
        "model, result = train_graph_classifier(model_name=\"GraphConv\",\n",
        "                                       c_hidden=256,\n",
        "                                       layer_name=\"GraphConv\",\n",
        "                                       num_layers=3,\n",
        "                                       dp_rate_linear=0.5,\n",
        "                                       dp_rate=0.0)\n",
        "\n",
        "# In kết quả cuối cùng\n",
        "print(f\"GraphConv Results:\")\n",
        "print_results(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_TuQFo2JSZP",
        "outputId": "01f6008f-2a4c-4fd6-978d-47b0283da6bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train performance: 94.27%\n",
            "Test performance:  92.11%\n"
          ]
        }
      ],
      "source": [
        "# In ra kết quả hiệu suất của mô hình trên tập Train và tập Test.\n",
        "print(f\"Train performance: {100.0*result['train']:4.2f}%\")\n",
        "print(f\"Test performance:  {100.0*result['test']:4.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpC6b_kvJSZP"
      },
      "source": [
        "**Test performance** cho thấy rằng chúng ta đạt được điểm số khá tốt trên một phần **unseen** (chưa từng thấy) của **dataset**.\n",
        "\n",
        "Cần lưu ý rằng vì chúng ta đã sử dụng **test set** cho cả **validation**, chúng ta có thể đã **overfitted** nhẹ vào tập này.\n",
        "\n",
        "Tuy nhiên, **experiment** cho thấy rằng **GNNs** thực sự mạnh mẽ để dự đoán các tính chất của **graphs** và/hoặc **molecules**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2oKPWF3JSZQ"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Trong **tutorial** này, chúng ta đã thấy ứng dụng của **neural networks** vào các **graph structures**.\n",
        "\n",
        "Chúng ta đã xem xét cách một **graph** có thể được biểu diễn (**adjacency matrix** hoặc **edge list**), và thảo luận về việc **implementation** các **graph layers** phổ biến: **GCN** và **GAT**. Các **implementations** đã cho thấy khía cạnh thực tế của các **layers**, điều mà thường dễ dàng hơn so với lý thuyết.\n",
        "\n",
        "Cuối cùng, chúng ta đã thực nghiệm với các **tasks** khác nhau, ở các cấp độ **node-**, **edge-** và **graph-level**.\n",
        "\n",
        "Nhìn chung, chúng ta đã thấy rằng việc bao gồm **graph information** trong các **predictions** có thể là cốt yếu để đạt được **high performance**. Có rất nhiều ứng dụng hưởng lợi từ **GNNs**, và tầm quan trọng của các mạng này có thể sẽ tăng lên trong những năm tới."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}